---
title: GOA FHS Assessment Bridging (version & data)
author: M Sosa Kapur maia.kapur@noaa.gov
date: November 2022
format: html
toc: true
always_allow_html: true
editor: source
---

# Bridging from SS3v3.24U to SSv3.30.17

C. Monnahan began this effort in summer of 2021. He found that the `ss_trans.exe` software was unable to work on the 2017 model as-was due to the following line in the .dat file: `-1 0 #_surveytiming_in_season`. R. Methot suggested converting this line to `-1.5  #_surveytiming_in_season`, which enabled the transition function to complete, but modified the timing of the survey, thus producing discrepant likelihoods. R. Methot then suggested changing the survey index month to 1.0 (in the new v3.30 dat file), but added that v3.24 could only produce expected length-composition values using midseason age-length keys. In other words, the previous version of this model was estimating length composition values assuming that the age-length key applied to the middle of the year, so there will necessarily be a discrepancy between the structure and magnitude of vulnerable biomass to the survey if those data are specified as being observed at the beginning of the year.

To work around this, Rick indicated we could instead adjust only the survey composition month ("`seas`") in the transitioned model to 7.0, essentially aligning the observed compositions with the mid-year biomass. We acknowledge this is a kludge, considering that the compositions and indices come from the same surveys; for future benchmark updates, scientists may elect to change the survey data season universally (in line with the model labeled `old-MLEs` below). This cycle, we elected to use the approach which produced results most similar to the 2017 benchmark.

The following documents the steps taken to explore this issue and arrive at a bridged baseline model in SS3v3.30.17. The likelihoods and derived quantities are similar (and indistinguishable for management purposes) but we do not believe values will ever be an exact match due to differences in how survey timing is handled between versions.

Note that the transitioning step otherwise conserved data weighting components (i.e., the McAllister-Ianelli values which upweight both fleet's lenghtcomps and downweight the survey CAALs)

```{r echo = FALSE, include = FALSE, warning = FALSE, message = FALSE}
require(r4ss)
require(here)
require(tidyverse)

models <- c('3.24U','3.30_old_mles', '3.30_adjust_timing', '3.30_converted')
colrs <- c('grey22',MetBrewer::met.brewer(name = 'Degas',n = 15))  #c('black',alpha('goldenrod',0.2),alpha('dodgerblue2',0.2),'seagreen4')
outlist <- lapply(paste0(here('model_runs','01_bridging','cole',models)),
                  function(x) c(model=x, SS_output(x, covar=FALSE, printstats=FALSE, verbose=FALSE)))
 
group1 <- SSsummarize(list(outlist[[1]],outlist[[2]],outlist[[3]]))
group2 <- SSsummarize(list(outlist[[1]],outlist[[2]],outlist[[3]],outlist[[4]])) 

## load 2017 model and fully converted model separately
m0_0 <- SS_output(here('model_runs','m0_0'), covar=TRUE, verbose=FALSE, printstats=FALSE) 
m2017 <- SS_output(here('model_runs','2017-mod'), covar=TRUE, verbose=FALSE, printstats=FALSE) 
```

## Transition with updated survey timing

### Survey index, length and CAAL month = 7.0

Take the `.ss_new` files from model 3.24U, update the survey index **and** composition timing to 7.0, and run `ss_trans.exe v3.30.17` using command `-maxI -1`. This command overrides the max phase set in the starter file (effectively disabling estimation).

## Pass old MLEs for rec devs

### Survey index, length and CAAL month = 7.0

This model takes the `.ss_new` files from the transitioned model, and pastes in the MLEs for recruitment devs from the original model (3.24) into the `.par` file (both recdev early and main). This exercise demonstrates that the population dynamics are the same when the forecasted recruitment devs from the original model (3.24U) are passed into the `.ss_new` files from the transitioned model.

## Adjust timing

### Survey index month = 1.0, length and CAAL month = 7.0

Using the `.ss_new` files from the previous model (with `3.24U`'s rec devs), turn off the estimation of forecasted recruitment devs (`forecast dev phase = -1` in starter), **change the survey index timing back to month = 1.0** and re-run using the original MLEs (`ss.par`, no estimation). This is effectively re-introducing the survey timing as specified in the original model while keeping all the optimized the parameters the same; the fit to the index does improve, but it is not an exact match. **This demonstrates that the discrepancies between versions is caused by the way survey timing is handled in the internal dynamics of v3.30+.**

## Optimize (full conversion)

This is the same as the previous model, but with optimization enabled. **This model ("converted", red line below) acts as the baseline transitioned model for 3.30.17.** After optimization, the index matches well, but it scales the population up ($log R_0$ increases from 12.8219 to 12.8948). However, note that steepness is fixed to 1 in this model, so this just increases the average recruitment.

```{r  include = T, echo = FALSE, warning = FALSE, message = FALSE}

SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=c(1,13))

```

Note that the vulnerable biomass (to the survey) has changed between versions, due to timing.

```{r   include = T, echo = FALSE, warning = FALSE, message = FALSE}
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=1)
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=16)

bio <- lapply(outlist, function(x)
  cbind(model=x$model, x$timeseries[,c('Yr','SpawnBio','Bio_all')])) %>% bind_rows
indices <- lapply(outlist, function(x)
  cbind(model=x$model, x$cpue[,c('Yr','Vuln_bio')])) %>% bind_rows

p1 <- ggplot(bio, aes(Yr, SpawnBio, color=model)) + 
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position =  'none')+
  scale_color_manual(values = colrs, labels = models)

p2 <- ggplot(bio, aes(Yr, Bio_all, color=model))+
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position = 'none')+
  scale_color_manual(values = colrs, labels = models)

p3 <- ggplot(indices, aes(Yr, Vuln_bio, color=model))  +
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position = 'bottom')+
  scale_color_manual(values = colrs, labels = models)

require(patchwork) 
(p1 | p2) /p3
```

Tabular comparison of index estimates across models.

```{r include = T, echo = FALSE, warning = FALSE, message = FALSE}
indices %>% mutate(model = basename(model)) %>% pivot_wider(c('Yr'), names_from='model', values_from='Vuln_bio') %>% kableExtra::kable(.) %>% kableExtra::kable_styling('striped')
```

Tabular comparison of likelihoods across models.

```{r include = T, echo = FALSE, warning = FALSE, message = FALSE}
likes <- lapply(outlist, function(x)
  data.frame(component=row.names(x$likelihoods_used), 
             model=basename(x$model), x$likelihoods_used)) %>%
  bind_rows %>% remove_rownames %>% arrange(component, model) %>% filter(abs(values)>.01)
likes%>% pivot_wider(c('component'), names_from='model', values_from='values') %>%
  select(component, '3.24U','3.30_old_mles','3.30_adjust_timing','3.30_converted' )%>% kableExtra::kable(.) %>% kableExtra::kable_styling('striped')
```

Tabular comparison of model scale (R0 estimates) across models.

```{r include = T, echo = FALSE, warning = FALSE, message = FALSE}
r0 <- lapply(outlist, function(x)
  cbind(model=basename(x$model), x$parameters[,c('Label','Value')])) %>%
  bind_rows %>% remove_rownames %>% filter(Label %in% c("SR_LN(R0)"))
r0 %>% pivot_wider(c('Label'), names_from='model', values_from='Value')%>% kableExtra::kable(.) %>% kableExtra::kable_styling('striped')
```

## Attempt survey index timing month = 7

### Survey index month = 7.0, length and CAAL month = 7.0

Out of curiosity, we investigated the impact of (correctly) specifying the survey index month as 7.0 and disabling optimization, using the parameter values from `3.30_converted` above. Total SSB is distinct among the three (with the updated timing model even more different), though the survey fits are indistinguishable.

```{r include = T, echo = FALSE, warning = FALSE, message = FALSE}
models <- c('3.24U','m02_3.30_converted','m03_update_timing','m04_update_ageing_error','m05_VAST_index')
## m01 = v3.24U and m02 = 3.30_converted above, just loaded individually now
m01  <- SS_output(here('model_runs','01_bridging','cole','m01_2017_3.24U'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m02 <- SS_output(here('model_runs','01_bridging','cole','m02_2017_3.30.17'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m03 <- SS_output(here('model_runs','01_bridging','cole','m03_update_timing'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m04 <- SS_output(here('model_runs','01_bridging','cole','m04_update_ageing_error'), covar=TRUE, verbose=FALSE, printstats=FALSE)
```

```{r  echo = FALSE, warning = FALSE, message = FALSE}
SSplotComparisons(SSsummarize(list(m01,m02,m03)), plot = T, uncertainty=TRUE, subplot = 1, legendlabels = models)

SSplotComparisons(SSsummarize(list(m01,m02,m03)), plot = T, uncertainty=TRUE, subplot = 13, legendlabels = models)

```

## VAST data

I received the model-based index with data through 2021 from GAP in Spring 2022. Since I plan to present this as a sensitivity to the proposed base model, we should also illustrate how the use of VAST would have affected the previous \[converted\] benchmark. This model is identical to `m02_3.30_converted` otherwise (the survey index timing is 1.0). As we might expect, the SSB is higher in the VAST version since the model-based inputs are higher than the design-based since \~2000. Reassuringly the dynamics are largely similar between converted models for the years where the VAST & design-based inputs are more alike.

![](C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/figs/2022-04-18-index_VASTvsDesign.png)

```{r include = T, echo = FALSE, warning = FALSE, message = FALSE}
m05 <- SS_output(here('model_runs','01_bridging','cole','m05_VAST_index'), covar=TRUE, verbose=FALSE, printstats=FALSE)
 
SSplotComparisons(SSsummarize(list(m01,m02,m05)),  plot = T, uncertainty=TRUE,  subplot = 1, legendlabels = models[c(1,2,5)])

SSplotComparisons(SSsummarize(list(m01,m02,m05)),  plot = T, uncertainty=TRUE, subplot = 13, legendlabels = models[c(1,2,5)])
```

# New aging error matrix in 2017 model `m0_4`

The SSC recommended exploration/addition of a new aging error matrix. To that end, we have a few different options, and it is desirable to illustrate how they affect the base model (no new data) before proceeding.

The present GOA aging error matrix actually comes from the BSAI assessment, and basically says that error increases linearly to a maximum at age 16. We do have read-replicate data for GOA FHS. We created several candidate models that either treated the individual readers separately (there are \~10 of them) or pooled the data (meaning the difference between a read by Delsa & Jon is equivalent to the difference between Tom & Mary). The best AIC was obtained by a pooled-data model which assumed constant bias and sigma across readers (bias is the different-integer age read, and sigma is the variation in true age). This is called `aic2p` in the `ageing_error/real_data` folder. The bias is otherwise identical between approaches.

```{r,  include = T, echo = FALSE, warning = FALSE, message = FALSE}
newerr <- t(read.csv(here("data","ageing_error", "real_data", "aic2p", "ss_format_reader 1.csv"))) %>%
data.frame()
names(newerr) <- newerr[1,]
newerr <- newerr[-1,]
png(here('figs','ageerr_compare.png'), width = 5,  height = 3, res = 520, unit = 'in')
with(m0_0$age_error_sd, 
     plot( type1 ~ age, type = 'l', col =  'grey22', 
                              ylim = c(0,3),     ylab = 'Type 1 Error', xlab = 'Age'))
with(newerr, lines( SD ~True_Age, pch = 19, col =  'blue', add = T))
legend('topleft', 
       legend = c('old matrix (BSAI)', 'new matrix (AIC)'),
       
       col = c('grey22','blue'), pch = 19)
dev.off()
# knitr::include_graphics(here("data", "ageing_error","real_data","aic2p","compplots.png"))

```

Here I run the converted model with the *new* aging error matrix from `aic2p`. This involved replacing the chunk in the `.dat` file with the `1 age err definition` and a line for the bias ("expected age" from the package output) and CV. **There is not a strong change in derived quantities.** The newer model has a better overall likelihood, which shows up in the lengths, ages, and recruitments.

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('3.24U (2017)','3.30_converted','3.30_newAgeErr')
m0_4age <- SS_output(here('model_runs','01_bridging','cole','m04_update_ageing_error'))
# SS_plots(m0_4age)
# SSplotComparisons(SSsummarize(list(m2017,m0_0,m0_4age)),  uncertainty=TRUE, subplot = 1, legendlabels = models)
# SSplotComparisons(SSsummarize(list(m0_0,m0_4age)),
#                   plot = FALSE, png = T,
#                   plotdir = here('model_runs','01_bridging','cole','m04_update_ageing_error'), 
#                   uncertainty=TRUE,  legendlabels = models[2:3])

knitr::include_graphics(here('model_runs','01_bridging','cole','m04_update_ageing_error',
                             "plots",
                             "comp_condAALfit_Andre_plotsflt2mkt0_page2.png"))
knitr::include_graphics(here('model_runs','01_bridging','cole','m04_update_ageing_error',
                             "plots",
                             "comp_condAALfit_Andre_plotsflt2mkt0_page3.png"))

ageErrcomps <- SSsummarize(list(m0_0,m0_4))
ageErrcomps$likelihoods %>% mutate(model2 < model1)



```

# Data Bridging

Here I start with the transitioned model and sequentially add data. Unless otherwise specified, the entire data series was replaced with the new extraction to accomodate for any changes in our databases. There are no changes made to the parameter specifications (priors, start values, etc). The following changes were also made to peripheral SS3 files before beginning; these apply to models `m0_1` onward:

I copied `m02_2017_3.30.17` into the main `model_runs` dir and renamed it to "m0_0".

-   In `control.ss`, the main period of recruitment devs apparently hasn't changed from 2010.1 upon the last assessment. For consistency I will leave that as-is for now, but likely will adjust as a sensitivity. However i did change `_end_yr_for_ramp_in_MPD` to 2026.4. Time blocks do not cross the endyr so I made no adjustments there.

-   In `data.ss` change `endyr` to 2022.

-   Ensure estimation is turned on in `starter.ss`

-   In `forecast.ss` first year for caps and allocations = 2023, update rebuilder years, though **note** we do not use the forecast file whatsoever in this assessment.

-   Functionality check was conducted in dir `/m0_0check` to confirm that time series estimation is pushed forward as expected.

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('m0_0','m0_0check')
m0_0check <- SS_output(here('model_runs','m0_0check'), covar=F, verbose=FALSE, printstats=FALSE)
SSplotComparisons(SSsummarize(list(m0_0,m0_0check)),  uncertainty=TRUE, subplot = 1, legendlabels = models)
SSplotComparisons(SSsummarize(list(m0_0,m0_0check)),  uncertainty=TRUE, subplot = 11, legendlabels = models)
```

## Add catch `m0_1`

2022 catches are a final estimate as of 13 Oct 2022.

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('2017 Base v3.30.16','+catch', '+survey biomass', '+fishery lengths','+survey lengths','+CAAL')
# m0_1 <- SS_output(here('model_runs','m0_1'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_1,    png = T, dir = here('model_runs','m0_1'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1 )),  plot = T, png = T, plotdir = here('model_runs','m0_1'), uncertainty=TRUE,  legendlabels = models[1:2])
knitr::include_graphics(here('model_runs','m0_1','plots','data_plot2.png'))
```

## Add design-based survey index `m0_2`

In addition to the 2019 & 2021 data points, this time series has a different value for 2001, about 18,000mt less than previous. Discussion with RACE and review of both the AFSC database and previous SAFE tables indicated we should go with the newer, lower value -- it is not clear where the \~170mt value came from previously.

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_2 <- SS_output(here('model_runs','m0_2'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_2, png = T, dir = here('model_runs','m0_2'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_2'), uncertainty=TRUE,  legendlabels = models[1:3], indexPlotEach = T)
m0_3 <- SS_output(here('model_runs','m0_3'), covar=TRUE, verbose=FALSE, printstats=FALSE)
```

## Add fishery length comps `m_03`

As previously, data before 1989 are included but turned off. The following years of the fishery length comps are ghosted: `r paste(unique(m0_3$ghostlendbase$Yr), collapse = ", ")` based on the 15-haul cutoff, which is slightly different than 2017 (2000 and 2008 are included now). I noticed that the input data in the previous model is in somewhat strange raw numbers (i.e an observed number of 132.708 within a sample size of 46). When you run `ss_output()` on the original data, it returns `lendbase` scaled to 1, which I used to spot check that the extracted data was accurate in the `00_getData.R` script. So, I replaced the entire data frame in this step, acknowledging that the input scale is different.

The aggregate fit is fine but the residuals in the early period are awful (this was the case before as well).

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
# SS_plots(m0_3,   png = T, dir =   here('model_runs','m0_3'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_3'), uncertainty=TRUE,  legendlabels =  models[1:4],indexPlotEach = T)
```

```{r, echo=FALSE, out.height = '25%'}
knitr::include_graphics(here('model_runs','m0_3','plots','comp_lenfit__aggregated_across_time.png'))
knitr::include_graphics(here('model_runs','m0_3','plots','comp_lenfit_residsflt1mkt0_page2.png'))
```

## Add survey length comps `m_04`

The nsamp (nhauls) has changed by 1 or so across years between the old version and this. I updated the data-prep method from Carey's code into `tidyverse` and spot checked that the values were consistent among entries (with a rounding tolerance of 1e-4). I also see that in the original data, data from years 1982-1988 are ignored, as are years 2000, 2002, and 2008 (for fishery); these are related to low-haul years. As above, this overwrites the entirety of the survey length comps in the `.dat` file with the values drawn from RACEBASE, Jul 2022.

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_4 <- SS_output(here('model_runs','m0_4'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_4,   png = T, dir =   here('model_runs','m0_4'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3,m0_4)),
# plot = FALSE, png = T, plotdir = here('model_runs','m0_4'), uncertainty=TRUE,  legendlabels = models[1:5],indexPlotEach = T)
```

## Add survey CAALs & marginal survey ages as ghost fleet `m0_5`

Here I incorporate the conditional ages at length as well as the marginal ages, both from the GOA survey thru year 2021; the latter are read in as a ghost fleet and don't contribute to the likelihoods. I confirmed with N Roberson that these were in RACEBASE before extracting to avoid issues with the `AGEPOP` column (this is an active effort zone with SMART/RACE reproductibility initiatives). For clarity, I deleted the expected values printed at the bottom of `data.ss` because in earlier runs I accidentally wrote the CAALs into there (they didn't show up in the data plots!).

As of `m_05`, all of the new data have been included in the model, but there hasn't been any changes to model process/spex beyond the end-year stuff mentioned above.

Also note that this model has not yet dealt with the various `warning` messages, and has retained the Francis weighting values from the benchmark. Notably, those values down-weight the age comp (CAAL) data to 0.34, which might explain why the diagnostics therein are only so-so.

Good to look at the SS.html file to confirm the marginal ages are indeed ghosted in there.

```{r include = TRUE, echo = FALSE, warning = FALSE, message = FALSE}
m0_5 <- SS_output(here('model_runs','m0_5'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_5,   png = T, dir =   here('model_runs','m0_5'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3,m0_4,m0_5)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_5'), uncertainty=TRUE,  legendlabels = models[1:6],indexPlotEach = T)

## view entirety of new data
par(mfrow = c(1,2))
SSplotData(m0_0)
SSplotData(m0_5)
```

## Warnings and further model updates `m0_6`

Now that the data are in, I do the minimal amount of tweaks to deal with the warnings and update the rec-dev situation here.`m0_6` has the same data as `m0_5`, but with the following changes in response to `warnings.ss`. After each change I double-checked that things could run but didn't make individual models for each change. + `control.ss`: Change recr_dist method to 4, removing recdist parameters below growth (no pop dy change) + `control.ss`: `last year of main recr_devs` to 2020. Update bias adjustment ramp in so that the `_last_yr_fullbias_adj_in_MPD` = `2020.1`, and `_end_yr_for_ramp_in_MPD` = 2021.4 (thus endyear is not in forecast. I didn't change the maximum bias adjustment value, but it seems a little low; the min-max recdevs also seem a little high.) + `control.ss`: Parameters 6 & 17 (`Age_DblN_end_logit_Fishery` & `Age_DblN_end_logit_Survey`) is fixed at 999 to make it asymptotic. This can typically be accomplished by values of \~20, so changing the max and init to 20, 15. Will double check these are still asymptotic afterwards. + `control.ss`:Par `InitF_seas_1_flt_1Fishery` is hitting low bound (estimated at 0.006, min at 0 - will leave as-is because it can't be below zero.) This warning will remain.

Remaining forecast warnings regard convergence, sigmaR ratios, and gradient. Will revisit these soon.

```{r, echo=FALSE}
models = append(models, "warnings")
m0_6 <- SS_output(here('model_runs','m0_6'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_6,   png = T, dir =   here('model_runs','m0_6'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3,m0_4,m0_5,m0_6)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_6'), uncertainty=TRUE,  legendlabels = models,indexPlotEach = T)
# SSplotSelex(m0_6, subplot = 2) ## slx still asymptotic 
```

# Sanity Checks and Explorations

Now that all the data are added and principal warnings dealt with let's do some sanity checks. It's evident that the survey data made the biggest change in the SSB trend, and did so in a logical direction. The 2017 base model erroneously uses a value of \~170k mt in the 2001 survey, which has since been revised/corrected down to about 150k mt.The new recdevs are mostly positive but with two down years, which makes sense given the recent survey.

```{r, echo=FALSE}
knitr::include_graphics("C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/compare2_spawnbio_uncertainty.png")
knitr::include_graphics("C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/compare11_recdevs.png")
```

## What survey data causes downshift? `m0_6b:d`

Here are some tests runs with the survey data:

-   `m0_6b`: this is `m0_6` **year 2001** reverted to the \~170,000k value which was used previously in year 2001, to see if downshifting this year bends the trend downards;
-   `m0_6c`: this is `m0_6` with survey data truncated back to 2017 (to see if the later years alone are causing the downshift, which would be sensible considering the trend), and
-   `m0_6d`: this is `m0_6` with both the changes mentioned above (revert year 2001 and remove values after 2017). This is generally equivalent to using the 2017 benchmark survey INDEX data, since all values besides 2001 were consistent between design-based survey pulls.

```{r include = F, echo = FALSE, warning = FALSE, message = FALSE}
# models = c("2017 base", "m0_6", "m0_6b [reverted 2001 survey]", 
#            "m0_6c [truncate survey]", "m0_6d [revert & truncate]")
# m0_6b <- SS_output(here('model_runs','m0_6b'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# m0_6c <- SS_output(here('model_runs','m0_6c'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# m0_6d <- SS_output(here('model_runs','m0_6d'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SSplotComparisons(SSsummarize(list(m0_0,m0_6,m0_6b,m0_6c,m0_6d)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_6d'), uncertainty=TRUE,  legendlabels = models, indexPlotEach = T)
```

```{r  echo = F, warning = FALSE, message = FALSE }
knitr::include_graphics("C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6d/compare2_spawnbio_uncertainty.png")
knitr::include_graphics("C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6d/compare13_indices.png")
```

This exercise demonstrates that a) the \~20 mt difference in the 2001 survey is *not* driving the changes between models, as `m0_6b` below is still downshifted, and b) truncation alone (`m0_6c`) is enough to get the trend back in line with the 2017 base model. Thus I do not plan to change individual years of survey data.

*However*, the fits to those down years is quite poor. I notice that fits to CAALs and lengths are quite good especially in recent years, but the CAAL sd diagnostic plots are *rough and worsening* for later years; my hope is this can get addressed via the aging error updates, and possibly by extending the main rec devs period.

Specifically, the observed CAALs are decently stable and well fit in recent years, but the SDs are huge and missed basically throughout (these plots are from `m0_5`), pointing to an aging error issue. Since the expected SDs are uniformly higher, it suggests that the model things there is more variance in age at length than what we've provided. Coupled with the fact that we have rec-devs disabled since 2012 (when the main period ends), we aren't allowing for the decreased observed index to be reflected in reduced recruitment output.

```{r  echo = F, warning = FALSE, message = FALSE }
knitr::include_graphics("C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/plots/comp_condAALfit_Andre_plotsflt2mkt0_page3.png")
knitr::include_graphics("C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/plots/comp_condAALfit_Andre_plotsflt2mkt0_page4.png")
```

## What happens when the main rec-dev period gets pushed out? `m0_6`

This model takes `m0_5` and extends the main rec-dev period to 2021 in the control file. Line 127 was shifted from 2012 to 2021, and line 136 (last year of full bias adjustment) shifted to 2019.1. This allows the model to take information from the next \~10 years of surveys/comps into account when calculating rec devs.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_5", "m0_6 [rec dev shift]")
m0_6 <- SS_output(here('model_runs','m0_6'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_6,   png = T, dir =   here('model_runs','m0_6'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_5,m0_6)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_6'), uncertainty=TRUE,  legendlabels = models, indexPlotEach = T)
```

Updating the rec-devs does not seem to cause better fits to the survey, nor does it indicate low recruitment in the years leading up to the low survey observations.While I think it makes sense to extend the main rec-dev period, this is clearly not enough to grant the flexibility required to better fit the recent years of survey data.

```{r  echo = F, warning = FALSE, message = FALSE }
knitr::include_graphics('C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/compare2_spawnbio_uncertainty.png')
knitr::include_graphics('C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/compare11_recdevs.png')
knitr::include_graphics('C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_6/compare13_indices.png')
```

## Can the model as-is better fit the VAST data? `m0_7`

This takes model `m0_6` and replaces the series with the model-based VAST data (thru 2021), which has a stabler, higher trend in recent years. This threw an overall gradient warning. Both models seem to want to over fit those later years, the VAST model to a greater degree. This means there's another data signal that's leading to the uptick, which the design-based survey is likely in conflict with. but I note that this issue was persistent in the 2017 model as well (basically overestimates years 2012+ in the index).

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_6", "m0_7 [VAST data instead]")
m0_7 <- SS_output(here('model_runs','m0_7'), covar=TRUE, verbose=FALSE, printstats=FALSE)
SS_plots(m0_7,   png = T, dir =   here('model_runs','m0_7'))
SSplotComparisons(SSsummarize(list(m0_0,m0_6,m0_7)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_7'), uncertainty=TRUE,  legendlabels = models, indexPlotEach = T)
# par(mfrow = c(1,2))
# SSplotIndices(m0_6, subplots = 2);SSplotIndices(m0_7, subplots = 2)
```

The model seems largely insensitive to the survey data, at least at the scale difference we're seeing here. The stability and upward trend in recent years is conserved at the expense of poor fits to the index. That said, there really is not a ton of information in our index, which means the model is likely relying more heavily on other information sources (namely lengths). I notice too that early lengths from the fishery are pretty overfit.

```{r  echo = F, warning = FALSE, message = FALSE }
knitr::include_graphics('C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_7/compare2_spawnbio_uncertainty.png')
knitr::include_graphics('C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_7/compare11_recdevs.png')
knitr::include_graphics('C:/Users/maia.kapur/Work/assessments/2022/goa-fhs-2022/model_runs/m0_7/compare13_indices.png')
```

# Finalize Base Model 
 

This section adds in the new aging error matrix, does a final pass on warning messages, and explores re-upping the tuning steps. Because this is my first time with this assessment, the benchmark I'd like to present this fall will have minimal process changes to the setup provided by Carey. As sensitivities, I will explore a few issues as mentioned in the last CIE (separate RMD). Note that `m0_6` is the bridged model with all new data except the aging error changes.

## Add New Aging Error Matrix `m0_8`  

Recall that we ran the 2017 model with the new aging error matrix (here `m0_4`) and didn't see a strong change in derived quants. For completeness and following the CIE, I recommend we use the (latest) info that we have. After running this model I pasted in the data_ss.new so the formatting was improved.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_6", "m0_4 (base w/ new ageerr)", "m0_8 (m0_6 w/ new ageerr)")
m0_8 <- SS_output(here('model_runs','m0_8'), covar=TRUE, verbose=FALSE, printstats=FALSE)
## double check error matrices are indeed distinct bw m06 and 4age, 8
m0_6$age_error_mean; m0_4age$age_error_mean; m0_8$age_error_mean
# SS_plots(m0_8,   png = T, dir =   here('model_runs','m0_8'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_6,m0_4age,m0_8)),  plot = T, png = T, plotdir = here('model_runs','m0_8'), uncertainty=TRUE,  legendlabels = models, indexPlotEach = T)
SSplotComparisons(SSsummarize(list(m0_0,m0_6,m0_4,m0_8)),   legendlabels = models, indexPlotEach = T, subplots = c(2,11,13))
```

## Revisit Data Weights
Through this point I haven't touched the Ianelli data weights at the end of the `.ctl` file (which were conserved thru transition). Clearly when this was run, tunings were allowed to exceed 1.0. Without running the model it seems to already have New MI weights suggested, which are even higher for the lengthcomps and lower for the ages. This suggests the nHauls input for the length comps are too low, and too high for the CAALs. Note that `SS_tune_comps` only does what the function name says; it won't suggest new upweights for the survey CV (for example).

First I do a run with the data weights reverted to 1 for the composition & survey biomass data to compare.  This is `m0_8c`.The model weights make a HUGE difference:
```{r echo = F, warning = FALSE, message = FALSE }
m0_8c <- SS_output(here('model_runs','m0_8c'), covar=TRUE, verbose=FALSE, printstats=FALSE)

SSplotComparisons(SSsummarize(list(m0_8,
                                   m0_8c)),
                  plotdir = here('model_runs','m0_8c'),
                  print =T,
                  col = c(baseCol,sensPal[4]),
                  # subplots = c(2,13),
                  legendlabels = c('m0_8 [putative base]',
                                   'm0_8 with no weights'))

```


Doing a cursory check to see if Francis weights are very different than MI at this point. The suggested MI values (when starting from nothing) are roughly in line with what's in `m0_8`; the Francis values, however, all are down-weighted instead (?).

Also note that the values vary slightly depending whether you start from m0_8 or m0_8c. Because starting from the former is basically doing a new set of tuning, I instead start from the unweighted model and use those values.

```{r echo = F, warning = FALSE, message = FALSE }
## just check out the values
tune_comps(m0_8c, 
              option = 'MI',
              niters_tuning = 0,
              allow_up_tuning = TRUE,
              write =F,
              dir = here('model_runs','m0_8c')) %>%
  select(Name, Type, New_MI, New_Francis)
```
Now I will do two more runs for thoroughness. Likely the base will use the new M-I values (`m0_8-newMI`) to be consistent with last time. But I also want to do a test with the suggested Francis runs (`m0_8-newFrancis`). The values were pasted from `m0_8c/suggested_tuning.ss` and run once.

The new MI values are in line with what was done before. The Francis values, however, seem to be responding to the survey. All weighted models seem to have a very different view of the stock than the unweighted model, and both Francis and MI agreed to downweight with Survey Ages, and weight the survey lengths the most out of the three. The unweighted model has many more poor recruitment events in recent years, leading to a different depletion trend.

I'm going to stick with the new M-I values for now. However, I think future models should use the Francis weights since they now play well with the CAALs, fit the survey better, and lead to a lower overall likelihood.

```{r echo = F, warning = FALSE, message = FALSE }
m0_8newMI <- SS_output(here('model_runs','m0_8-newMI'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m0_8newFrancis <- SS_output(here('model_runs','m0_8-newFrancis'), covar=TRUE, verbose=FALSE, printstats=FALSE)

SSplotComparisons(SSsummarize(list(m2017,
                                   # base_model,
                                   m0_8,
                                   # m0_8c,
                                   m0_8newMI,
                                   m0_8newFrancis)),
                  print = T,
                  plotdir =here('model_runs','m0_8-newFrancis'),
                  # subplots = c(2,4,11,13),
                  legendlabels = c('m2017',
                                   'm0_8',
                                   # 'm0_8c [no weights]',
                                   'm0_8 new MI',
                                   'm0_8 new Francis'))

cbind(m0_8newMI$likelihoods_used,m0_8newFrancis$likelihoods_used)
```

## Revisit bias adjustment 
Now that we have the new data weights, to deal with the warning "Main recdev biasadj is >2 times ratio of rmse to sigmaR", I will go with what comes out of `SS_fitbiasramp`. This suggests new breakpoints and maximum value for the ramp. This recommends *retaining the endyr for the ramp around 2015*, likely in response to the availability of the comp data. 


I took a look at the rmse_table. This shows the mean bias adjustment during each era; generally, we'd want to see the estimated sigma-R to approach our input. The warning means that the RMSE of the estimated rec-devs (in the report file) is very small (bias-adj is greater than rmse-squared over sigma-r-squared). I'm pretty sure the ideal offset value is given in eq 19 in the SS manual, and it's the third column of the rmse table here (0.168711 for MPD). 

After running, we can see that the mean bias adj now is much more in line with the ratio for the main period.

This did away with the bias adjustment warning. 

Both models have `InitF_seas_1_flt_1Fishery`  on bounds, and warnings about convergence in FMSY which I'm not too worried about since we derive those using a difference model instead. They also both have slightly high convergence (enough to throw warning). This goes away if the rec-dev bias ramp is pushed out but because that effectively upweights the comps data (against the recommendation of the weights) I'm going to leave it alone.

The BASE MODEL is now `m0_8-newMI-biasAdj`.
```{r echo = F, warning = FALSE, message = FALSE }

newBy <- SS_fitbiasramp(m0_8newMI, verbose = FALSE)
round(newBy$df$value[1:4], 1);round(newBy$df$value[5], 5)
m0_8newMI$rmse_table

m0_8newMIbias <- SS_output(here::here('model_runs','m0_8-newMI-biasAdj'), covar=TRUE, verbose=FALSE, printstats=FALSE)
SS_fitbiasramp(m0_8newMIbias, verbose = TRUE)
ggsave(last_plot(), file = here::here("bias_adj_panel.png"), width = 8, height = 10, unit = 'in', dpi = 520)
# m0_8newMIbias$rmse_table
# SSplotComparisons(SSsummarize(list(m2017, m0_8,m0_8newMI,m0_8newMIbias)),
#                   subplots = c(2,4,11,13),
#                   legendlabels = c('m2017',
#                                    'm0_8', 
#                                    'm0_8 new MI',
#                                     'm0_8 new MI + reduce bias'))

```


## Ignored runs
### Increase SigmaR limit `m0_8b`

To deal with warnings about biasadj, increase `_max_bias_adj_in_MPD` to 1.6. This actually made the gradient worse and didn't change the biasadj. 

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c("2017 base", "m0_8", "m0_8b (m0_8b biasadj to 1.6)")
m0_8b <- SS_output(here('model_runs','m0_8'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_8b, png = T, dir =   here('model_runs','m0_8b'))
# SSplotComparisons(SSsummarize(list(m0_0,m0_8,m0_8b)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_8b'), uncertainty=TRUE,  legendlabels = models)
```

