---
title: "GOA FHS Assessment Bridging (version & data)"
author: "M Sosa Kapur maia.kapur@noaa.gov"
output: html_notebook
---
# Bridging from SS3v3.24U to SSv3.30.17
C. Monnahan began this effort in summer of 2021. He found that the `ss_trans.exe` software was unable to work on the 2017 model as-was due to the following line in the .dat file:
`-1 0 #_surveytiming_in_season`.  R. Methot suggested converting this line to `-1.5  #_surveytiming_in_season`, which enabled the transition function to complete,
but modified the timing of the survey, thus producing discrepant likelihoods. R. Methot then suggested changing the survey index month to 1.0 (in the new v3.30 dat file), but added that v3.24 could only produce expected length-composition values using midseason age-length keys. In other words, the previous version of this model was estimating length composition values assuming that the age-length key applied to the middle of the year, so there will necessarily be a discrepancy between the structure and magnitude of vulnerable biomass to the survey if those data are specified as being observed at the beginning of the year.

To work around this, Rick indicated we could instead adjust only the survey composition month ("seas") to 7.0, essentially aligning the compositional observations with the mid-year biomass. We acknowledge this is a kludge considering the compositions and indices come from the same surveys; for future benchmark updates, scientists may elect to change the survey data season universally (in line with the model labeled `old-MLEs` below). This cycle, we elected to use the approach which produced results most similar to the 2017 benchmark.

The following documents the steps taken to explore this issue and arrive at a bridged baseline model in SS3v3.30.17. The likelihoods and derived quantities are similar (and indistinguishable for management purposes) but we do not believe values will ever be an exact match due to differences in how survey timing is handled between versions.
```{r, include = FALSE, warning = FALSE, message = FALSE}
require(r4ss)
require(here)
require(tidyverse)

models <- c('3.24U','3.30_old_mles', '3.30_adjust_timing', '3.30_converted')
colrs <- c('black',alpha('goldenrod',0.2),alpha('dodgerblue2',0.2),'seagreen4')
outlist <- lapply(paste0(here('model_runs','01_bridging','cole',models)),
                  function(x) c(model=x, SS_output(x, covar=FALSE, printstats=FALSE, verbose=FALSE)))
 
group1 <- SSsummarize(list(outlist[[1]],outlist[[2]],outlist[[3]]))
group2 <- SSsummarize(list(outlist[[1]],outlist[[2]],outlist[[3]],outlist[[4]])) 
```


## Transition with updated survey timing
### Survey index, length and CAAL month = 7.0 
Take the `.ss_new` files from model 3.24U, update the survey index **and** composition timing to 7.0, and run `ss_trans.exe v3.30.17` using command `-maxI -1`. This command overrides the max phase set in the starter file (effectively disabling estimation).

## Pass old MLEs for rec devs
### Survey index, length and CAAL month = 7.0 
This model takes the `.ss_new` files from the transitioned model, and pastes in the MLEs for recruitment devs from the original model (3.24) into the `.par` file (both recdev early and main). This exercise demonstrates that the population dynamics are the same when the forecasted recruitment devs from the original model (3.24U) are passed into the `.ss_new` files from the transitioned model.
 
## Adjust timing
### Survey index month = 1.0, length and CAAL month = 7.0 
Using the `.ss_new` files from the previous model (with `3.24U`'s rec devs), turn of the estimation of forecasted recruitment devs (`forecast dev phase = -1` in starter), **change the survey index timing back to month = 1.0** and re-run using the original MLEs (`ss.par`, no estimation). This is effectively re-introducing the survey timing as specified in the original model while keeping all the optimized the parameters the same; the fit to the index does improve, but it is not an exact match. **This demonstrates that the discrepancies between versions is caused by the way survey timing is handled in the internal dynamics of v3.30+.**
 

## Optimize (full conversion)
This is the same as the previous model, but with optimization enabled. **This model ("converted", green line below) acts as the baseline transitioned model for 3.30.17.** After optimization, the index matches well, but it scales the population up ($R_0$ increases from 12.8219 to 12.8948).  

```{r, echo = F, warning = FALSE, message = FALSE}

SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=1)

```

```{r, echo = F, warning = FALSE, message = FALSE}
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=13)
```
Note that the vulnerable biomass (to the survey) has changed between versions, due to timing.
```{r, echo = F, warning = FALSE, message = FALSE}
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=1)
SSplotComparisons(group2,plot  = T, legendlabels=models,
                  uncertainty=FALSE, subplots=16)
bio <- lapply(outlist, function(x)
  cbind(model=x$model, x$timeseries[,c('Yr','SpawnBio','Bio_all')])) %>% bind_rows
indices <- lapply(outlist, function(x)
  cbind(model=x$model, x$cpue[,c('Yr','Vuln_bio')])) %>% bind_rows

p1 <- ggplot(bio, aes(Yr, SpawnBio, color=model)) + 
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position =  'none')+
  scale_color_manual(values = colrs, labels = models)

p2 <- ggplot(bio, aes(Yr, Bio_all, color=model))+
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position = 'none')+
  scale_color_manual(values = colrs, labels = models)

p3 <- ggplot(indices, aes(Yr, Vuln_bio, color=model))  +
  geom_line(lwd = 1) + geom_point() + ylim(0,NA) + 
  ggsidekick::theme_sleek(base_size = 14) + theme(legend.position = 'bottom')+
  scale_color_manual(values = colrs, labels = models)

require(patchwork) 
(p1 | p2) /p3
```

Tabular comparison of index estimates across models.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
indices %>% mutate(model = basename(model)) %>% pivot_wider(c('Yr'), names_from='model', values_from='Vuln_bio')
```

Tabular comparison of likelihoods across models.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
likes <- lapply(outlist, function(x)
  data.frame(component=row.names(x$likelihoods_used), 
             model=basename(x$model), x$likelihoods_used)) %>%
  bind_rows %>% remove_rownames %>% arrange(component, model) %>% filter(abs(values)>.01)
likes%>% pivot_wider(c('component'), names_from='model', values_from='values') %>%
  select(component, '3.24U','3.30_old_mles','3.30_adjust_timing','3.30_converted' )
```
Tabular comparison of model scale (R0 estimates) across models.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
r0 <- lapply(outlist, function(x)
  cbind(model=basename(x$model), x$parameters[,c('Label','Value')])) %>%
  bind_rows %>% remove_rownames %>% filter(Label %in% c("SR_LN(R0)"))
r0 %>% pivot_wider(c('Label'), names_from='model', values_from='Value')
```


 
## Attempt survey index timing month  = 7
### Survey index month = 7.0, length and CAAL month = 7.0 

Out of curiosity, we investigated the impact of (correctly) specifying the survey index month as 7.0 and disabling optimization, using the parameter values from `3.30_converted` above. Total SSB is distinct among the three (with the updated timing model even more different), though the survey fits are indistinguishable.
```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
models <- c('3.24U','m02_3.30_converted','m03_update_timing','m04_update_ageing_error','m05_VAST_index')
## m01 = v3.24U and m02 = 3.30_converted above, just loaded individually now
m01 <- m0_0 <- SS_output(here('model_runs','01_bridging','cole','m01_2017_3.24U'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m02 <- SS_output(here('model_runs','01_bridging','cole','m02_2017_3.30.17'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m03 <- SS_output(here('model_runs','01_bridging','cole','m03_update_timing'), covar=TRUE, verbose=FALSE, printstats=FALSE)
m04 <- SS_output(here('model_runs','01_bridging','cole','m04_update_ageing_error'), covar=TRUE, verbose=FALSE, printstats=FALSE)

SSplotComparisons(SSsummarize(list(m01,m02,m03)),  uncertainty=TRUE, subplot = 1, legendlabels = models)
SSplotComparisons(SSsummarize(list(m01,m02,m03)),  uncertainty=TRUE, subplot = 13, legendlabels = models)
```


## VAST data 
I received the model-based index with data through 2021 from GAP in Spring 2022. Since I plan to present this as a sensitivity to the proposed base model, we should also illustrate how the use of VAST would have affected the previous [converted] benchmark. This model is identical to `m02_3.30_converted` otherwise (the survey index timing is 1.0). As we might expect, the SSB is higher in the VAST version since the model-based inputs are higher than the design-based since ~2000. Reassuringly the dynamics are largely similar between converted models for the years where the VAST & design-based inputs are more alike.


```{r pressure, echo=FALSE, out.width = '40%'}
knitr::include_graphics(here('figs','2022-04-18-index_VASTvsDesign.png'))
```


```{r, include = T, echo = FALSE, warning = FALSE, message = FALSE}
m05 <- SS_output(here('model_runs','01_bridging','cole','m05_VAST_index'), covar=TRUE, verbose=FALSE, printstats=FALSE)
SSplotComparisons(SSsummarize(list(m01,m02,m05)),  uncertainty=TRUE, subplot = 1, legendlabels = models[c(1,2,5)])
SSplotComparisons(SSsummarize(list(m01,m02,m05)),  uncertainty=TRUE, subplot = 13, legendlabels = models[c(1,2,5)])
```

# New aging error matrix
The SSC recommended exploration/addition of a new aging error matrix. To that end, we have a few different options, and it is desirable to illustrate how they affect the base model (no new data) before proceeding.

# Data Bridging 
I copied `m02_2017_3.30.17` into the main `model_runs` dir and renamed it to "m0_0".
Here I start with the transitioned model and sequentially add data. Unless otherwise specified, the entire data series was replaced with the new extraction to accomodate for any changes in our databases. There are no changes made to the parameter specifications (priors, start values, etc).
The following changes were also made to peripheral SS3 files before beginning; these apply to models `m0_1` onwards:

+ In `control.ss`, the main period of recruitment devs apparently hasn't changed from 2012 upon the last assessment. For consistency I will leave that as-is for now, but likely will adjust as a sensitivity. However, I did change `_end_yr_for_ramp_in_MPD` to 2026.4. Time blocks do not cross the endyr so I made no adjustments there.

+ In `data.ss` change `endyr` to 2022.

+ Ensure estimation is turned on in `starter.ss`

+ In `forecast.ss` first year for caps and allocations = 2023, update rebuilder years, though **note** we do not use the forecast file whatsoever in this assessment.

+ Functionality check was conducted in dir `/m0_0check` to confirm that time series estimation is pushed forward as expected.

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('m0_0','m0_0check')
m0_0check <- SS_output(here('model_runs','m0_0check'), covar=TRUE, verbose=FALSE, printstats=FALSE)
SSplotComparisons(SSsummarize(list(m0_0,m0_0check)),  uncertainty=TRUE, subplot = 1, legendlabels = models)
SSplotComparisons(SSsummarize(list(m0_0,m0_0check)),  uncertainty=TRUE, subplot = 11, legendlabels = models)
```

## Add catch `m0_1`
Right now 2022 catches have the 2021 numbers as a placeholder, will project closer to the time of.
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
models = c('2017 Base','+catch', '+survey', '+fish_len','+goa_len','+CAAL')
m0_1 <- SS_output(here('model_runs','m0_1'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_1,    png = T, dir = here('model_runs','m0_1'))
SSplotComparisons(SSsummarize(list(m0_0,m0_1 )),  plot = FALSE, png = T, plotdir = here('model_runs','m0_1'), uncertainty=TRUE,  legendlabels = models)
```
## Add design-based survey index `m0_2`
In addition to the 2019 & 2021 datapoints, this time series has a different value for 2001, about 18mt less than previous. Discussion with RACE and review of both the AFSC database and previous SAFE tables indicated we should go with this value -- it is not clear where the ~170mt value came from previously.
```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_2 <- SS_output(here('model_runs','m0_2'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_2, png = T, dir = here('model_runs','m0_2'))
SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_2'), uncertainty=TRUE,  legendlabels = models)
```
## Add fishery length comps `m_03`
As previously, data before 1989 are included but turned off. I noticed that the input data in the previous model is in somewhat strange raw numbers (i.e an observed number of 132.708 within a sample size of 46). When you run `ss_output()` on the original data, it returns `lendbase` scaled to 1, which I used to spot check that the extracted data was accurate in the `00_getData.R` script. So, I replaced the entire data frame in this step, acknowledging that the input scale is different.

The aggregate fit is OK but the residuals in the early period are awful (this was the case before as well).

```{r, include = F, echo = FALSE, warning = FALSE, message = FALSE}
m0_3 <- SS_output(here('model_runs','m0_3'), covar=TRUE, verbose=FALSE, printstats=FALSE)
# SS_plots(m0_3,   png = T, dir =   here('model_runs','m0_3'))
SSplotComparisons(SSsummarize(list(m0_0,m0_1,m0_2,m0_3)),  plot = FALSE, png = T, plotdir = here('model_runs','m0_3'), uncertainty=TRUE,  legendlabels = models)
# SSplotComps(m0_3, fleets = 1, kind = 'LEN', subplot = 21)
# SSplotComps(m0_3, fleets = 1, kind = 'LEN', subplot = 24)
```
```{r, echo=FALSE, out.height = '25%'}
knitr::include_graphics(here('model_runs','m0_3','plots','comp_lenfit__aggregated_across_time.png'))
knitr::include_graphics(here('model_runs','m0_3','plots','comp_lenfit_residsflt1mkt0_page2.png'))
```

## Add survey length comps `m_04`
The nsamp (nhauls) has changed by 1 or so across years between the old version and this.


## Add CAALs (plus marginal ghost fleet)



